{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM76OPoAXJT2HubkGkrTC5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohmdumer/Spam_-_No_Spam_Emails/blob/main/Spam_%26_no_spam_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfHg5KbclhIj",
        "outputId": "4f038c1f-145e-4efc-86d1-8271c38105eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/content/spam_or_not_spam.csv')\n",
        "\n",
        "# Display basic info about the DataFrame\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STXA3Ures4ML",
        "outputId": "983e61a5-2c4d-4fb0-bfc3-c7dea7de701a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   email   2999 non-null   object\n",
            " 1   label   3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 47.0+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxwFk0PltpfN",
        "outputId": "b4ef38f4-69df-4121-c246-2d50463f311a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               email  label\n",
            "0   date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...      0\n",
            "1  martin a posted tassos papadopoulos the greek ...      0\n",
            "2  man threatens explosion in moscow thursday aug...      0\n",
            "3  klez the virus that won t die already the most...      0\n",
            "4   in adding cream to spaghetti carbonara which ...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaWITrqUwlr3",
        "outputId": "0e2637ed-3b13-43ea-8bf5-ad7160dc488f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "email    1\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove null values\n",
        "\n",
        "df = df.dropna()\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjC4wbgwwzmP",
        "outputId": "1cc1301f-a5bd-4c83-cc52-5e4d5f792434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "email    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuxg82CV4yOk",
        "outputId": "6ed98eea-8d12-4483-9fc9-319866f3ed87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    2500\n",
            "1     499\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "-htWiQxFqKXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercasing\n",
        "# Removing punctuation, special characters, and extra spaces.\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df['cleaned_text'] = df['email'].apply(preprocess_text)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad1evYsgp8FQ",
        "outputId": "8cfc51ed-3283-4afc-e9bf-64b1ca534fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               email  label  \\\n",
            "0   date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...      0   \n",
            "1  martin a posted tassos papadopoulos the greek ...      0   \n",
            "2  man threatens explosion in moscow thursday aug...      0   \n",
            "3  klez the virus that won t die already the most...      0   \n",
            "4   in adding cream to spaghetti carbonara which ...      0   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  date wed number aug number number number numbe...  \n",
            "1  martin a posted tassos papadopoulos the greek ...  \n",
            "2  man threatens explosion in moscow thursday aug...  \n",
            "3  klez the virus that won t die already the most...  \n",
            "4  in adding cream to spaghetti carbonara which h...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigrams and Bigrams"
      ],
      "metadata": {
        "id": "o8w9kC3YxyKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and n-grams generation\n",
        "def generate_ngrams(text, n):\n",
        "    tokens = word_tokenize(text)\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "# Generating unigrams and bigrams\n",
        "df['unigrams'] = df['cleaned_text'].apply(lambda x: generate_ngrams(x, 1))\n",
        "df['bigrams'] = df['cleaned_text'].apply(lambda x: generate_ngrams(x, 2))\n"
      ],
      "metadata": {
        "id": "Hy8yQybJqq_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enx4Sj_H7cwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Dataset and Apply SMOTE"
      ],
      "metadata": {
        "id": "K4x81gPj7f0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# Convert text into feature vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
        "X = vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
        "y = df['label']\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Convert back to DataFrame for n-gram analysis\n",
        "resampled_data = pd.DataFrame(X_smote, columns=vectorizer.get_feature_names_out())\n",
        "resampled_data['label'] = y_smote\n",
        "\n",
        "# Check the balanced distribution\n",
        "print(resampled_data['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I37sC9XL80mR",
        "outputId": "d5d1b471-201f-4445-a6b9-e55e1070fe6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    2500\n",
            "1    2500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Probabilities"
      ],
      "metadata": {
        "id": "aSuY2IKYyxgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate n-gram probabilities\n",
        "def calculate_ngram_probabilities(df, labels, ngram_type):\n",
        "    spam_ngrams = Counter()\n",
        "    non_spam_ngrams = Counter()\n",
        "\n",
        "    for i, text in enumerate(data):\n",
        "        ngrams_list = generate_ngrams(text, ngram_type)\n",
        "        if labels[i] == 1:  # Spam\n",
        "            spam_ngrams.update(ngrams_list)\n",
        "        else:  # Non-Spam\n",
        "            non_spam_ngrams.update(ngrams_list)\n",
        "\n",
        "    total_spam = sum(spam_ngrams.values())\n",
        "    total_non_spam = sum(non_spam_ngrams.values())\n",
        "\n",
        "    spam_probs = {ngram: (count + 1) / (total_spam + len(spam_ngrams)) for ngram, count in spam_ngrams.items()}\n",
        "    non_spam_probs = {ngram: (count + 1) / (total_non_spam + len(non_spam_ngrams)) for ngram, count in non_spam_ngrams.items()}\n",
        "\n",
        "    return spam_probs, non_spam_probs\n",
        "\n",
        "# Recalculate probabilities\n",
        "unigram_probs_spam, unigram_probs_non_spam = calculate_ngram_probabilities(df['cleaned_text'], df['label'], 1)\n",
        "bigram_probs_spam, bigram_probs_non_spam = calculate_ngram_probabilities(df['cleaned_text'], df['label'], 2)\n"
      ],
      "metadata": {
        "id": "0lmpVgCmy0Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Prediction Function\n",
        "# The function will:\n",
        "\n",
        "# Preprocess the input sentence.\n",
        "# Generate unigrams and bigrams.\n",
        "# Compute posterior probabilities.\n",
        "# Predict the class with the highest posterior probability.\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_class(sentence, unigram_probs_spam, unigram_probs_non_spam, bigram_probs_spam, bigram_probs_non_spam):\n",
        "    sentence = preprocess_text(sentence)\n",
        "    unigrams = generate_ngrams(sentence, 1)\n",
        "    bigrams = generate_ngrams(sentence, 2)\n",
        "\n",
        "    spam_prob = 1\n",
        "    non_spam_prob = 1\n",
        "    alpha = 1  # Laplace smoothing\n",
        "\n",
        "    # Calculate unigram probabilities\n",
        "    for unigram in unigrams:\n",
        "        spam_prob *= unigram_probs_spam.get(unigram, alpha)\n",
        "        non_spam_prob *= unigram_probs_non_spam.get(unigram, alpha)\n",
        "\n",
        "    # Calculate bigram probabilities\n",
        "    for bigram in bigrams:\n",
        "        spam_prob *= bigram_probs_spam.get(bigram, alpha)\n",
        "        non_spam_prob *= bigram_probs_non_spam.get(bigram, alpha)\n",
        "\n",
        "    # Normalize probabilities\n",
        "    total_prob = spam_prob + non_spam_prob\n",
        "    spam_posterior = spam_prob / total_prob\n",
        "    non_spam_posterior = non_spam_prob / total_prob\n",
        "\n",
        "    # Predict the class\n",
        "    predicted_class = 1 if spam_posterior > non_spam_posterior else 0\n",
        "\n",
        "    return predicted_class, spam_posterior, non_spam_posterior\n",
        "\n"
      ],
      "metadata": {
        "id": "J7tShRhpzCXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the Function\n",
        "# Test the function with three user-defined sentences.\n",
        "\n",
        "# Test the function\n",
        "sentences = [\n",
        "    \"Congratulations Mr. Muhammad Umer Naseem! Your visa for United States of America has been approved.\"\n",
        "    \"Congratulations! You've won a free ticket.\",\n",
        "    \"Can we schedule a meeting for tomorrow?\",\n",
        "    \"This is your last chance to claim the offer.\",\n",
        "    \"Mr. Muhammad Umer Naseem! you have win a free ticket. Congratulations!\",\n",
        "    \"Hi John, can we meet for lunch tomorrow at 1 PM?\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    predicted_class, spam_prob, non_spam_prob = predict_class(\n",
        "        sentence,\n",
        "        unigram_probs_spam,\n",
        "        unigram_probs_non_spam,\n",
        "        bigram_probs_spam,\n",
        "        bigram_probs_non_spam\n",
        "    )\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predicted Class: {'Spam' if predicted_class == 1 else 'Not Spam'}\")\n",
        "    print(f\"Spam Probability: {spam_prob:.4f}, Non-Spam Probability: {non_spam_prob:.4f}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RKkRu_Azbnt",
        "outputId": "832a5ed8-7365-4258-aaf8-1174a55e8756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Congratulations Mr. Muhammad Umer Naseem! Your visa for United States of America has been approved.Congratulations! You've won a free ticket.\n",
            "Predicted Class: Spam\n",
            "Spam Probability: 0.9027, Non-Spam Probability: 0.0973\n",
            "\n",
            "Sentence: Can we schedule a meeting for tomorrow?\n",
            "Predicted Class: Spam\n",
            "Spam Probability: 0.9027, Non-Spam Probability: 0.0973\n",
            "\n",
            "Sentence: This is your last chance to claim the offer.\n",
            "Predicted Class: Not Spam\n",
            "Spam Probability: 0.5000, Non-Spam Probability: 0.5000\n",
            "\n",
            "Sentence: Mr. Muhammad Umer Naseem! you have win a free ticket. Congratulations!\n",
            "Predicted Class: Spam\n",
            "Spam Probability: 0.9027, Non-Spam Probability: 0.0973\n",
            "\n",
            "Sentence: Hi John, can we meet for lunch tomorrow at 1 PM?\n",
            "Predicted Class: Not Spam\n",
            "Spam Probability: 0.5000, Non-Spam Probability: 0.5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPeg4lUuzxJu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}